{"cells":[{"cell_type":"markdown","metadata":{"id":"TBFXQGKYUc4X"},"source":["##### Copyright 2022 The TensorFlow Authors.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","executionInfo":{"elapsed":4,"status":"ok","timestamp":1714989868414,"user":{"displayName":"Dmitry Korotkov","userId":"03375750349352320128"},"user_tz":-180},"id":"1z4xy2gTUc4a"},"outputs":[],"source":["# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"KwQtSOz0VrVX"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/video/video_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/video/video_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/video/video_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/video/video_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"L2MHy42s5wl6"},"source":["# Классификация видео с помощью 3D-сверточной нейронной сети\n","\n","В этом руководстве демонстрируется обучение трехмерной сверточной нейронной сети (CNN) для классификации видео с использованием набора данных распознавания действий [UCF101](https://www.crcv.ucf.edu/data/UCF101.php). 3D CNN использует трехмерный фильтр для выполнения сверток. Ядро может скользить в трех направлениях, тогда как в 2D CNN оно может скользить в двух измерениях. Модель основана на работе, опубликованной в книге «Более детальный взгляд на пространственно-временные свертки для распознавания действий» (https://arxiv.org/abs/1711.11248v3) Д. Трана и др. (2017). В этом уроке вы:\n","\n","- Построить входной конвейер\n","- Постройте 3D-модель сверточной нейронной сети с остаточными связями, используя функциональный API Keras.\n","- Обучите модель\n","- Оценить и протестировать модель\n","\n","Это видеоурок по классификации является второй частью серии видеоуроков по TensorFlow. Вот еще три урока:\n","\n","- [Загрузка видеоданных](https://www.tensorflow.org/tutorials/load_data/video): в этом руководстве объясняется большая часть кода, используемого в этом документе.\n","- [MoViNet для распознавания потоковой передачи] (https://www.tensorflow.org/hub/tutorials/movinet): ознакомьтесь с моделями MoViNet, доступными в TF Hub.\n","- [Перенос обучения для классификации видео с помощью MoViNet] (https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet): в этом руководстве объясняется, как использовать предварительно обученную модель классификации видео, обученную на другом наборе данных с помощью UCF- 101 набор данных.\n"]},{"cell_type":"markdown","metadata":{"id":"_Ih_df2q0kw4"},"source":["## Настройка\n","\n","Начните с установки и импорта некоторых необходимых библиотек, в том числе:\n","[remotezip](https://github.com/gtsystem/python-remotezip), чтобы проверить содержимое ZIP-файла, [tqdm](https://github.com/tqdm/tqdm), чтобы использовать индикатор выполнения, [ OpenCV](https://opencv.org/) для обработки видеофайлов, [einops](https://github.com/arogozhnikov/einops/tree/master/docs) для выполнения более сложных тензорных операций и [`tensorflow_docs `](https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs) для встраивания данных в блокнот Jupyter.\n","\n","**Примечание**. Для запуска этого руководства используйте TensorFlow 2.10. Версии выше TensorFlow 2.10 могут работать некорректно.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":88002,"status":"ok","timestamp":1714989956413,"user":{"displayName":"Dmitry Korotkov","userId":"03375750349352320128"},"user_tz":-180},"id":"KEbL4Mwi01PV","outputId":"d0ab1caa-3a27-42f7-a512-a4f38d372fcc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting remotezip\n","  Downloading remotezip-0.12.3-py3-none-any.whl (8.1 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from remotezip) (2.31.0)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->remotezip) (2024.2.2)\n","Installing collected packages: einops, remotezip\n","Successfully installed einops-0.8.0 remotezip-0.12.3\n","Collecting tensorflow==2.10.0\n","  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.3.25)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10.0)\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.63.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.9.0)\n","Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10.0)\n","  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.10.0)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.0)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10.0)\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n","Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10.0)\n","  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.37.0)\n","Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10.0)\n","  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (4.11.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.14.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n","Installing collected packages: tensorboard-plugin-wit, keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.15.0\n","    Uninstalling tensorflow-estimator-2.15.0:\n","      Successfully uninstalled tensorflow-estimator-2.15.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.4\n","    Uninstalling gast-0.5.4:\n","      Successfully uninstalled gast-0.5.4\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.0\n","    Uninstalling google-auth-oauthlib-1.2.0:\n","      Successfully uninstalled google-auth-oauthlib-1.2.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n","tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"bd430e65d308415fa137b47bdeecdb56","pip_warning":{"packages":["google"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install remotezip tqdm opencv-python einops\n","!pip install tensorflow==2.10.0"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4522,"status":"ok","timestamp":1714989960918,"user":{"displayName":"Dmitry Korotkov","userId":"03375750349352320128"},"user_tz":-180},"id":"gg0otuqb0hIf"},"outputs":[],"source":["import tqdm\n","import random\n","import pathlib\n","import itertools\n","import collections\n","\n","import cv2\n","import einops\n","import numpy as np\n","import remotezip as rz\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import keras\n","from keras import layers"]},{"cell_type":"markdown","metadata":{"id":"Ctk9A57-6ABq"},"source":["## Загрузка и предварительная обработка видеоданных\n","\n","Скрытая ячейка ниже определяет вспомогательные функции для загрузки фрагмента данных из набора данных UCF-101 и загрузки его в tf.data.Dataset. Подробнее о конкретных этапах предварительной обработки можно узнать в [Учебном руководстве по загрузке видеоданных](../load_data/video.ipynb), где этот код рассматривается более подробно.\n","\n","Класс FrameGenerator в конце скрытого блока является здесь самой важной утилитой. Он создает итерируемый объект, который может передавать данные в конвейер данных TensorFlow. В частности, этот класс содержит генератор Python, который загружает видеокадры вместе с их закодированной меткой. Функция генератора (`__call__`) возвращает массив кадров, созданный с помощью `frames_from_video_file`, и вектор метки, закодированный в горячем виде, связанный с набором кадров.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714989960918,"user":{"displayName":"Dmitry Korotkov","userId":"03375750349352320128"},"user_tz":-180},"id":"nB2aOTU35r9_"},"outputs":[],"source":["# @title\n","\n","\n","def list_files_per_class(zip_url):\n","    \"\"\"\n","    Перечислите файлы в каждом классе набора данных по URL-адресу zip-архива.\n","\n","         Аргументы:\n","           zip_url: URL-адрес, с которого можно распаковать файлы.\n","\n","         Возвращаться:\n","           files: Список файлов в каждом из классов.\n","    \"\"\"\n","    files = []\n","    with rz.RemoteZip(URL) as zip:\n","        for zip_info in zip.infolist():\n","            files.append(zip_info.filename)\n","    return files\n","\n","\n","def get_class(fname):\n","    \"\"\"\n","    Получить имя класса по имени файла.\n","\n","         Аргументы:\n","           fname: Имя файла в наборе данных UCF101.\n","\n","         Возвращаться:\n","           Класс, которому принадлежит файл.\n","    \"\"\"\n","    return fname.split(\"_\")[-3]\n","\n","\n","def get_files_per_class(files):\n","    \"\"\"\n","    Получите файлы, принадлежащие каждому классу.\n","\n","         Аргументы:\n","           files: список файлов в наборе данных.\n","\n","         Возвращаться:\n","           Словарь имен классов (ключ) и файлов (значения).\n","    \"\"\"\n","    files_for_class = collections.defaultdict(list)\n","    for fname in files:\n","        class_name = get_class(fname)\n","        files_for_class[class_name].append(fname)\n","    return files_for_class\n","\n","\n","def download_from_zip(zip_url, to_dir, file_names):\n","    \"\"\"\n","    Загрузите содержимое zip-файла по URL-адресу zip.\n","\n","         Аргументы:\n","           zip_url: URL-адрес архива, содержащий данные.\n","           to_dir: каталог для загрузки данных.\n","           file_names: Имена файлов для загрузки.\n","    \"\"\"\n","    with rz.RemoteZip(zip_url) as zip:\n","        for fn in tqdm.tqdm(file_names):\n","            class_name = get_class(fn)\n","            zip.extract(fn, str(to_dir / class_name))\n","            unzipped_file = to_dir / class_name / fn\n","\n","            fn = pathlib.Path(fn).parts[-1]\n","            output_file = to_dir / class_name / fn\n","            unzipped_file.rename(\n","                output_file,\n","            )\n","\n","\n","def split_class_lists(files_for_class, count):\n","    \"\"\"\n","    Возвращает список файлов, принадлежащих подмножеству данных, а также остальную часть\n","         файлы, которые необходимо скачать.\n","\n","         Аргументы:\n","           files_for_class: файлы, принадлежащие определенному классу данных.\n","           count: количество файлов для загрузки.\n","\n","         Возвращаться:\n","           Split_files: файлы, принадлежащие подмножеству данных.\n","           remainder: словарь остатка файлов, которые необходимо загрузить.\n","    \"\"\"\n","    split_files = []\n","    remainder = {}\n","    for cls in files_for_class:\n","        split_files.extend(files_for_class[cls][:count])\n","        remainder[cls] = files_for_class[cls][count:]\n","    return split_files, remainder\n","\n","\n","def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n","    \"\"\"\n","    Загрузите подмножество набора данных UFC101 и разделите его на различные части, например\n","       обучение, валидация и тестирование.\n","\n","       Аргументы:\n","         zip_url: URL-адрес архива, содержащий данные.\n","         num_classes: количество меток.\n","         splits: словарь, определяющий обучение, проверку, тестирование и т. д. (ключевое) разделение данных.\n","                 (значение — количество файлов на разделение).\n","         download_dir: Каталог для загрузки данных.\n","    \"\"\"\n","    files = list_files_per_class(zip_url)\n","    for f in files:\n","        tokens = f.split(\"/\")\n","        if len(tokens) <= 2:\n","            files.remove(f)  # Remove that item from the list if it does not have a filename\n","\n","    files_for_class = get_files_per_class(files)\n","\n","    classes = list(files_for_class.keys())[:num_classes]\n","\n","    for cls in classes:\n","        new_files_for_class = files_for_class[cls]\n","        random.shuffle(new_files_for_class)\n","        files_for_class[cls] = new_files_for_class\n","\n","    # Only use the number of classes you want in the dictionary\n","    files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n","\n","    dirs = {}\n","    for split_name, split_count in splits.items():\n","        print(split_name, \":\")\n","        split_dir = download_dir / split_name\n","        split_files, files_for_class = split_class_lists(files_for_class, split_count)\n","        download_from_zip(zip_url, split_dir, split_files)\n","        dirs[split_name] = split_dir\n","\n","    return dirs\n","\n","\n","def format_frames(frame, output_size):\n","    \"\"\"\n","    Заполните и измените размер изображения из видео.\n","\n","       Аргументы:\n","         frame: изображение, размер которого необходимо изменить и дополнить.\n","         output_size: размер пикселя изображения выходного кадра.\n","\n","       Возвращаться:\n","         Отформатированный кадр с заполнением указанного выходного размера.\n","    \"\"\"\n","    frame = tf.image.convert_image_dtype(frame, tf.float32)\n","    frame = tf.image.resize_with_pad(frame, *output_size)\n","    return frame\n","\n","\n","def frames_from_video_file(video_path, n_frames, output_size=(224, 224), frame_step=15):\n","    \"\"\"\n","    Создает кадры из каждого видеофайла, присутствующего в каждой категории.\n","\n","       Аргументы:\n","         video_path: путь к файлу видео.\n","         n_frames: количество кадров, которые будут созданы для каждого видеофайла.\n","         output_size: размер пикселя изображения выходного кадра.\n","\n","       Возвращаться:\n","         Массив кадров NumPy в форме (n_frames, высота, ширина, каналы).\n","    \"\"\"\n","    # Read each video frame by frame\n","    result = []\n","    src = cv2.VideoCapture(str(video_path))\n","\n","    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n","\n","    need_length = 1 + (n_frames - 1) * frame_step\n","\n","    if need_length > video_length:\n","        start = 0\n","    else:\n","        max_start = video_length - need_length\n","        start = random.randint(0, max_start + 1)\n","\n","    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n","    # ret is a boolean indicating whether read was successful, frame is the image itself\n","    ret, frame = src.read()\n","    result.append(format_frames(frame, output_size))\n","\n","    for _ in range(n_frames - 1):\n","        for _ in range(frame_step):\n","            ret, frame = src.read()\n","        if ret:\n","            frame = format_frames(frame, output_size)\n","            result.append(frame)\n","        else:\n","            result.append(np.zeros_like(result[0]))\n","    src.release()\n","    result = np.array(result)[..., [2, 1, 0]]\n","\n","    return result\n","\n","\n","class FrameGenerator:\n","    def __init__(self, path, n_frames, training=False):\n","        \"\"\"Возвращает набор кадров со связанной с ними меткой.\n","\n","        Аргументы:\n","          path: пути к видеофайлам.\n","          n_frames: количество кадров.\n","          training: логическое значение, позволяющее определить, создается ли набор обучающих данных.\n","        \"\"\"\n","        self.path = path\n","        self.n_frames = n_frames\n","        self.training = training\n","        self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n","        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n","\n","    def get_files_and_class_names(self):\n","        video_paths = list(self.path.glob(\"*/*.avi\"))\n","        classes = [p.parent.name for p in video_paths]\n","        return video_paths, classes\n","\n","    def __call__(self):\n","        video_paths, classes = self.get_files_and_class_names()\n","\n","        pairs = list(zip(video_paths, classes))\n","\n","        if self.training:\n","            random.shuffle(pairs)\n","\n","        for path, name in pairs:\n","            video_frames = frames_from_video_file(path, self.n_frames)\n","            label = self.class_ids_for_name[name]  # Encode labels\n","            yield video_frames, label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYY7PkdJFM4Z","outputId":"fe6fc69b-092c-49cc-93ef-96034412af19"},"outputs":[{"name":"stdout","output_type":"stream","text":["train :\n"]},{"name":"stderr","output_type":"stream","text":[" 55%|█████▍    | 164/300 [00:32<00:58,  2.33it/s]"]}],"source":["URL = \"https://storage.googleapis.com/thumos14_files/UCF101_videos.zip\"\n","download_dir = pathlib.Path(\"./UCF101_subset/\")\n","subset_paths = download_ufc_101_subset(URL, num_classes=10, splits={\"train\": 30, \"val\": 10, \"test\": 10}, download_dir=download_dir)"]},{"cell_type":"markdown","metadata":{"id":"C0O3ttIzpFZJ"},"source":["Create the training, validation, and test sets (`train_ds`, `val_ds`, and `test_ds`).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lq86IyGDJjTX"},"outputs":[],"source":["n_frames = 10\n","batch_size = 8\n","\n","output_signature = (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32), tf.TensorSpec(shape=(), dtype=tf.int16))\n","\n","train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths[\"train\"], n_frames, training=True), output_signature=output_signature)\n","\n","\n","# Batch the data\n","train_ds = train_ds.batch(batch_size)\n","\n","val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths[\"val\"], n_frames), output_signature=output_signature)\n","val_ds = val_ds.batch(batch_size)\n","\n","test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths[\"test\"], n_frames), output_signature=output_signature)\n","\n","test_ds = test_ds.batch(batch_size)"]},{"cell_type":"markdown","metadata":{"id":"nzogoGA4pQW0"},"source":["## Создаём модель\n","\n","Следующая трехмерная модель сверточной нейронной сети основана на статье [Более внимательный взгляд на пространственно-временные свертки для распознавания действий](https://arxiv.org/abs/1711.11248v3) Д. Трана и др. (2017). В статье сравниваются несколько версий 3D ResNets. Вместо того, чтобы работать с одним изображением с размерами «(высота, ширина)», как стандартные сети ResNet, они работают с объемом видео «(время, высота, ширина)». Самый очевидный подход к этой проблеме — заменить каждую 2D-свертку («layers.Conv2D») на 3D-свертку («layers.Conv3D»).\n","\n","В этом руководстве используется свертка (2 + 1)D с [остаточными соединениями](https://arxiv.org/abs/1512.03385). Свертка (2 + 1)D позволяет разложить пространственные и временные измерения, создавая, таким образом, два отдельных шага. Преимущество этого подхода заключается в том, что факторизация сверток по пространственным и временным измерениям экономит параметры.\n","\n","Для каждого выходного местоположения трехмерная свертка объединяет все векторы из трехмерного фрагмента объема для создания одного вектора в выходном объеме.\n","\n","![3D-извилины](https://www.tensorflow.org/images/tutorials/video/3DCNN.png)\n","\n","Эта операция принимает входные данные «время _ высота _ ширина _ каналы» и создает выходные данные «каналы» (при условии, что количество входных и выходных каналов одинаково. Таким образом, 3D-слой свертки с размером ядра `(3 x 3 x 3 )` потребуется весовая матрица с элементами `27 _ каналов \\*\\* 2`. В справочном документе обнаружено, что более эффективным и действенным подходом является факторизация свертки вместо одной трехмерной свертки для обработки измерений времени и пространства. они предложили свертку «(2+1)D», которая обрабатывает измерения пространства и времени отдельно. На рисунке ниже показаны факторизованные пространственные и временные свертки (2 + 1)D свертки.\n","\n","![(2+1)D свертки](https://www.tensorflow.org/images/tutorials/video/2plus1CNN.png)\n","\n","Основное преимущество этого подхода в том, что он уменьшает количество параметров. В свертке (2 + 1)D пространственная свертка принимает данные формы «(1, ширина, высота)», а временная свертка принимает данные формы «(время, 1, 1)». Например, для свертки (2 + 1)D с размером ядра `(3 x 3 x 3)` потребуются весовые матрицы размера `(9 * каналов**2) + (3 * каналов**2)`, меньше вдвое меньше, чем при полной 3D-свертке. В этом руководстве реализована (2 + 1)D ResNet18, где каждая свертка в сети заменяется сверткой (2+1)D.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZcB_7dg-EZJ"},"outputs":[],"source":["# Define the dimensions of one frame in the set of frames created\n","HEIGHT = 224\n","WIDTH = 224"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yD_sDIBlNu7K"},"outputs":[],"source":["class Conv2Plus1D(keras.layers.Layer):\n","    def __init__(self, filters, kernel_size, padding):\n","        \"\"\"\n","        Последовательность сверточных слоев, которые сначала применяют операцию свертки к\n","        пространственные измерения, а затем и временные измерения.\n","        \"\"\"\n","        super().__init__()\n","        self.seq = keras.Sequential(\n","            [\n","                # Spatial decomposition\n","                layers.Conv3D(filters=filters, kernel_size=(1, kernel_size[1], kernel_size[2]), padding=padding),\n","                # Temporal decomposition\n","                layers.Conv3D(filters=filters, kernel_size=(kernel_size[0], 1, 1), padding=padding),\n","            ]\n","        )\n","\n","    def call(self, x):\n","        return self.seq(x)"]},{"cell_type":"markdown","metadata":{"id":"I-fCAddqEORZ"},"source":["Модель ResNet состоит из последовательности остаточных блоков.\n","Остаточный блок имеет две ветви. Основная ветвь выполняет расчет, но через нее трудно пройти градиентам.\n","Остаточная ветвь обходит основные вычисления и в основном просто добавляет входные данные к выходным данным основной ветви.\n","Градиенты легко проходят через эту ветвь.\n","Следовательно, будет существовать простой путь от функции потерь к любой основной ветви остаточного блока.\n","Это позволяет избежать проблемы исчезающего градиента.\n","\n","Создайте основную ветвь остаточного блока со следующим классом. В отличие от стандартной структуры ResNet, здесь используется собственный слой Conv2Plus1D вместо Layers.Conv2D.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjxAKHwn6mTJ"},"outputs":[],"source":["class ResidualMain(keras.layers.Layer):\n","    \"\"\"\n","    Остаточный блок модели со сверткой, нормализацией слоев и\n","    функция активации, ReLU.\n","    \"\"\"\n","\n","    def __init__(self, filters, kernel_size):\n","        super().__init__()\n","        self.seq = keras.Sequential(\n","            [\n","                Conv2Plus1D(filters=filters, kernel_size=kernel_size, padding=\"same\"),\n","                layers.LayerNormalization(),\n","                layers.ReLU(),\n","                Conv2Plus1D(filters=filters, kernel_size=kernel_size, padding=\"same\"),\n","                layers.LayerNormalization(),\n","            ]\n","        )\n","\n","    def call(self, x):\n","        return self.seq(x)"]},{"cell_type":"markdown","metadata":{"id":"CevmZ9qsdpWC"},"source":["Чтобы добавить остаточную ветвь к основной, она должна иметь тот же размер. Уровень «Project» ниже предназначен для случаев, когда количество каналов изменяется в ветке. В частности, добавляется последовательность плотносвязных слоев с последующей нормализацией.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znrk5BrL6kuq"},"outputs":[],"source":["class Project(keras.layers.Layer):\n","    \"\"\"\n","    Спроецируйте определенные размеры тензора, когда данные проходят через разные\n","    фильтры по размеру и пониженная дискретизация.\n","    \"\"\"\n","\n","    def __init__(self, units):\n","        super().__init__()\n","        self.seq = keras.Sequential([layers.Dense(units), layers.LayerNormalization()])\n","\n","    def call(self, x):\n","        return self.seq(x)"]},{"cell_type":"markdown","metadata":{"id":"S8zycXGvfnak"},"source":["Используйте `add_residual_block`, чтобы ввести пропускное соединение между слоями модели.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urjVgqvw-TlB"},"outputs":[],"source":["def add_residual_block(input, filters, kernel_size):\n","    \"\"\"\n","    Добавьте в модель остаточные блоки. Если последние измерения входных данных\n","    и размер фильтра не совпадает, спроецируйте его так, чтобы последний размер совпадал.\n","    \"\"\"\n","    out = ResidualMain(filters, kernel_size)(input)\n","\n","    res = input\n","    # Using the Keras functional APIs, project the last dimension of the tensor to\n","    # match the new filter size\n","    if out.shape[-1] != input.shape[-1]:\n","        res = Project(out.shape[-1])(res)\n","\n","    return layers.add([res, out])"]},{"cell_type":"markdown","metadata":{"id":"bozog_0hFKrD"},"source":["Изменение размера видео необходимо для выполнения субдискретизации данных. В частности, понижение разрешения видеокадров позволяет модели проверять определенные части кадров, чтобы обнаружить закономерности, которые могут быть характерны для определенного действия. Посредством понижения дискретизации несущественная информация может быть отброшена. Более того, изменение размера видео позволит уменьшить размерность и, следовательно, ускорить обработку модели.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQOWuc2I-QqK"},"outputs":[],"source":["class ResizeVideo(keras.layers.Layer):\n","    def __init__(self, height, width):\n","        super().__init__()\n","        self.height = height\n","        self.width = width\n","        self.resizing_layer = layers.Resizing(self.height, self.width)\n","\n","    def call(self, video):\n","        \"\"\"\n","        Аргументы:\n","                 video: Тензорное представление видео в виде набора кадров.\n","\n","               Возвращаться:\n","                 Уменьшенный размер видео в соответствии с новой высотой и шириной, до которых оно должно быть изменено.\n","        \"\"\"\n","        # b stands for batch size, t stands for time, h stands for height,\n","        # w stands for width, and c stands for the number of channels.\n","        old_shape = einops.parse_shape(video, \"b t h w c\")\n","        images = einops.rearrange(video, \"b t h w c -> (b t) h w c\")\n","        images = self.resizing_layer(images)\n","        videos = einops.rearrange(images, \"(b t) h w c -> b t h w c\", t=old_shape[\"t\"])\n","        return videos"]},{"cell_type":"markdown","metadata":{"id":"Z9IqzCq--Uu9"},"source":["Используйте [функциональный API Keras](https://www.tensorflow.org/guide/keras/functional) для создания остаточной сети.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bROfh_K-Wxs"},"outputs":[],"source":["input_shape = (None, 10, HEIGHT, WIDTH, 3)\n","input = layers.Input(shape=(input_shape[1:]))\n","x = input\n","\n","x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding=\"same\")(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.ReLU()(x)\n","x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n","\n","# Block 1\n","x = add_residual_block(x, 16, (3, 3, 3))\n","x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n","\n","# Block 2\n","x = add_residual_block(x, 32, (3, 3, 3))\n","x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n","\n","# Block 3\n","x = add_residual_block(x, 64, (3, 3, 3))\n","x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n","\n","# Block 4\n","x = add_residual_block(x, 128, (3, 3, 3))\n","\n","x = layers.GlobalAveragePooling3D()(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(10)(x)\n","\n","model = keras.Model(input, x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TiO0WylG-ZHM"},"outputs":[],"source":["frames, label = next(iter(train_ds))\n","model.build(frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAsKrM8r-bKM"},"outputs":[],"source":["# Visualize the model\n","keras.utils.plot_model(model, expand_nested=True, dpi=60, show_shapes=True)"]},{"cell_type":"markdown","metadata":{"id":"1yvJJPnY-dMP"},"source":["## Обучите модель\n","\n","Для этого урока выберите оптимизатор tf.keras.optimizers.Adam и функцию потерь tf.keras.losses.SparseCategoricalCrossentropy. Используйте аргумент «метрики», чтобы просмотреть точность производительности модели на каждом этапе.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejrbyebDp2tA"},"outputs":[],"source":["model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])"]},{"cell_type":"markdown","metadata":{"id":"nZT1Xlx9stP2"},"source":["Обучите модель в течение 50 эпох с помощью метода Keras Model.fit.\n","\n","Примечание. Этот пример модели обучен на меньшем количестве точек данных (300 обучающих и 100 проверочных примеров), чтобы обеспечить разумное время обучения для этого руководства. Более того, обучение этой примерной модели может занять более часа.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMrMUl2hOqMs"},"outputs":[],"source":["history = model.fit(x=train_ds, epochs=7, validation_data=val_ds)"]},{"cell_type":"markdown","metadata":{"id":"KKUfMNVns2hu"},"source":["### Визуализируйте результаты\n","\n","Создайте графики потерь и точности на наборах обучения и проверки:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cd5tpNrtOrs7"},"outputs":[],"source":["def plot_history(history):\n","    \"\"\"\n","    Построение кривых обучения и проверки.\n","\n","    Аргументы:\n","       история: история модели со всеми метрическими показателями\n","    \"\"\"\n","    fig, (ax1, ax2) = plt.subplots(2)\n","\n","    fig.set_size_inches(18.5, 10.5)\n","\n","    # Plot loss\n","    ax1.set_title(\"Loss\")\n","    ax1.plot(history.history[\"loss\"], label=\"train\")\n","    ax1.plot(history.history[\"val_loss\"], label=\"test\")\n","    ax1.set_ylabel(\"Loss\")\n","\n","    # Determine upper bound of y-axis\n","    max_loss = max(history.history[\"loss\"] + history.history[\"val_loss\"])\n","\n","    ax1.set_ylim([0, np.ceil(max_loss)])\n","    ax1.set_xlabel(\"Epoch\")\n","    ax1.legend([\"Train\", \"Validation\"])\n","\n","    # Plot accuracy\n","    ax2.set_title(\"Accuracy\")\n","    ax2.plot(history.history[\"accuracy\"], label=\"train\")\n","    ax2.plot(history.history[\"val_accuracy\"], label=\"test\")\n","    ax2.set_ylabel(\"Accuracy\")\n","    ax2.set_ylim([0, 1])\n","    ax2.set_xlabel(\"Epoch\")\n","    ax2.legend([\"Train\", \"Validation\"])\n","\n","    plt.show()\n","\n","\n","plot_history(history)"]},{"cell_type":"markdown","metadata":{"id":"EJrGF0Sss8E0"},"source":["## Оцените модель\n","\n","Используйте Keras Model.evaluate, чтобы получить потери и точность в тестовом наборе данных.\n","\n","Примечание. В примере модели в этом руководстве используется подмножество набора данных UCF101, чтобы обеспечить разумное время обучения. Точность и потери можно улучшить за счет дальнейшей настройки гиперпараметров или дополнительных обучающих данных.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hev0hMCxOtfy"},"outputs":[],"source":["model.evaluate(test_ds, return_dict=True)"]},{"cell_type":"markdown","metadata":{"id":"-F73GxD1-yc8"},"source":["Чтобы дополнительно визуализировать производительность модели, используйте [матрицу путаницы](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix). Матрица путаницы позволяет оценить эффективность модели классификации за пределами точности. Чтобы построить матрицу путаницы для этой задачи классификации нескольких классов, получите фактические значения в тестовом наборе и прогнозируемые значения.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yw-6rG5V-0L-"},"outputs":[],"source":["def get_actual_predicted_labels(dataset):\n","    \"\"\"\n","    Создайте список фактических значений истинности и прогнозов модели.\n","\n","    Аргументы:\n","       набор данных: повторяемая структура данных, такая как набор данных TensorFlow, с функциями и метками.\n","\n","    Возвращаться:\n","       Основная истина и прогнозируемые значения для конкретного набора данных.\n","    \"\"\"\n","    actual = [labels for _, labels in dataset.unbatch()]\n","    predicted = model.predict(dataset)\n","\n","    actual = tf.stack(actual, axis=0)\n","    predicted = tf.concat(predicted, axis=0)\n","    predicted = tf.argmax(predicted, axis=1)\n","\n","    return actual, predicted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aln6qWW_-2dk"},"outputs":[],"source":["def plot_confusion_matrix(actual, predicted, labels, ds_type):\n","    cm = tf.math.confusion_matrix(actual, predicted)\n","    ax = sns.heatmap(cm, annot=True, fmt=\"g\")\n","    sns.set(rc={\"figure.figsize\": (12, 12)})\n","    sns.set(font_scale=1.4)\n","    ax.set_title(\"Confusion matrix of action recognition for \" + ds_type)\n","    ax.set_xlabel(\"Predicted Action\")\n","    ax.set_ylabel(\"Actual Action\")\n","    plt.xticks(rotation=90)\n","    plt.yticks(rotation=0)\n","    ax.xaxis.set_ticklabels(labels)\n","    ax.yaxis.set_ticklabels(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfQ3VAGd-4Az"},"outputs":[],"source":["fg = FrameGenerator(subset_paths[\"train\"], n_frames, training=True)\n","labels = list(fg.class_ids_for_name.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ucGpbiA-5qi"},"outputs":[],"source":["actual, predicted = get_actual_predicted_labels(train_ds)\n","plot_confusion_matrix(actual, predicted, labels, \"training\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mfr7AT5T-7ZD"},"outputs":[],"source":["actual, predicted = get_actual_predicted_labels(test_ds)\n","plot_confusion_matrix(actual, predicted, labels, \"test\")"]},{"cell_type":"markdown","metadata":{"id":"FefzeIZz-9aI"},"source":["Значения точности и полноты для каждого класса также можно рассчитать с использованием матрицы путаницы.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dq95-56Z-_E2"},"outputs":[],"source":["def calculate_classification_metrics(y_actual, y_pred, labels):\n","    \"\"\"\n","    Рассчитайте точность и полноту модели классификации, используя основные истинные данные и\n","         прогнозируемые значения.\n","\n","         Аргументы:\n","           y_actual: Ярлыки основной истины.\n","           y_pred: предсказанные метки.\n","           labels: список классификационных меток.\n","\n","         Возвращаться:\n","           Меры точности и отзыва.\n","    \"\"\"\n","    cm = tf.math.confusion_matrix(y_actual, y_pred)\n","    tp = np.diag(cm)  # Diagonal represents true positives\n","    precision = dict()\n","    recall = dict()\n","    for i in range(len(labels)):\n","        col = cm[:, i]\n","        fp = np.sum(col) - tp[i]  # Sum of column minus true positive is false negative\n","\n","        row = cm[i, :]\n","        fn = np.sum(row) - tp[i]  # Sum of row minus true positive, is false negative\n","\n","        precision[labels[i]] = tp[i] / (tp[i] + fp)  # Precision\n","\n","        recall[labels[i]] = tp[i] / (tp[i] + fn)  # Recall\n","\n","    return precision, recall"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jSEonYQ_BZt"},"outputs":[],"source":["precision, recall = calculate_classification_metrics(actual, predicted, labels)  # Test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXvTW1Df_DV8"},"outputs":[],"source":["precision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"be1yrQl5_EYF"},"outputs":[],"source":["recall"]},{"cell_type":"markdown","metadata":{"id":"d4WsP4Z2HZ6L"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TBFXQGKYUc4X"],"provenance":[{"file_id":"1aP378gRFLNROK1lhQQgarvYBR3Xv1vnO","timestamp":1714989995476},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/video/video_classification.ipynb","timestamp":1713962323362}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
