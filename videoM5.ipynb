{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Интеллектуальные методы обработки видео\n",
    "\n",
    "## Модуль 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Константы\n",
    "dataFolder = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стерео видео\n",
    "\n",
    "https://docs.luxonis.com/en/latest/\n",
    "\n",
    "https://github.com/isl-org/MiDaS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "c:\\git\\MUIV\\video_pocessing\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\facebookresearch_WSL-Images_main\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# Загрузка модели MiDaS для оценки глубины из PyTorch Hub\n",
    "model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "model.eval()\n",
    "\n",
    "# Если у вас есть GPU, разкомментируйте следующую строку для ускорения\n",
    "# model.to(torch.device(\"cuda\"))\n",
    "\n",
    "from torchvision.transforms.functional import pad\n",
    "\n",
    "\n",
    "def resize_and_pad(img, base=32):\n",
    "    \"\"\"Resize the image to a size that is a multiple of `base`, then pad if necessary.\"\"\"\n",
    "    w, h = img.size\n",
    "    new_w = ((w - 1) // base + 1) * base\n",
    "    new_h = ((h - 1) // base + 1) * base\n",
    "    resized_img = T.Resize((new_h, new_w))(img)\n",
    "    # Calculate padding\n",
    "    pad_left = pad_top = 0\n",
    "    pad_right = new_w - w\n",
    "    pad_bottom = new_h - h\n",
    "    padded_img = pad(resized_img, (pad_left, pad_top, pad_right, pad_bottom))\n",
    "    return padded_img\n",
    "\n",
    "\n",
    "# Обработка кадра для совместимости с моделью MiDaS\n",
    "def process_frame(frame, model):\n",
    "    # Convert the color from BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # Custom resize to ensure dimension compatibility\n",
    "    desired_size = 384  # Example size, adjust as necessary\n",
    "    frame_resized = frame_pil.resize((desired_size, desired_size), Image.BICUBIC)\n",
    "\n",
    "    # Prepare the transforms\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    input_tensor = transform(frame_resized).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Move input data to the GPU if available\n",
    "    input_tensor = input_tensor.to(next(model.parameters()).device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Model prediction\n",
    "        prediction = model(input_tensor)\n",
    "\n",
    "        # Resize prediction to original frame size\n",
    "        depth_map = torch.nn.functional.interpolate(prediction.unsqueeze(1), size=frame.shape[:2], mode=\"bicubic\", align_corners=False).squeeze()\n",
    "        depth_map = depth_map.cpu().numpy()\n",
    "\n",
    "    # Normalize depth map for visualization\n",
    "    depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    depth_map_normalized = depth_map_normalized.astype(np.uint8)\n",
    "    return depth_map_normalized\n",
    "\n",
    "\n",
    "# Чтение видео\n",
    "cap = cv2.VideoCapture(\"./data/src/Mating.mp4\")\n",
    "frame_count = 0\n",
    "\n",
    "# while True:\n",
    "for i in range(10):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Получение и сохранение карты глубины\n",
    "    depth_map = process_frame(frame, model)\n",
    "    cv2.imwrite(f\"./data/result/frame_{frame_count:04d}.png\", depth_map)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\facebookresearch_WSL-Images_main\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import pad\n",
    "\n",
    "# Загрузка модели MiDaS для оценки глубины из PyTorch Hub\n",
    "model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "model.eval()\n",
    "\n",
    "# Перенос модели на GPU для ускорения, если доступно\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "def process_frame(frame, model):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # Подготовка изображения\n",
    "    frame_resized = resize_and_pad(frame_pil)\n",
    "\n",
    "    # Преобразование и нормализация кадра\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    input_tensor = transform(frame_resized).unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(next(model.parameters()).device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "        depth_map = torch.nn.functional.interpolate(prediction.unsqueeze(1), size=frame.shape[:2], mode=\"bicubic\", align_corners=False).squeeze()\n",
    "        depth_map = depth_map.cpu().numpy()\n",
    "\n",
    "    # Нормализация карты глубины для визуализации\n",
    "    depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    depth_map_normalized = depth_map_normalized.astype(np.uint8)\n",
    "    return depth_map_normalized\n",
    "\n",
    "\n",
    "def resize_and_pad(img, base=32, desired_size=384):\n",
    "    # Изменение размера изображения с учетом базового размера\n",
    "    w, h = img.size\n",
    "    new_w, new_h = desired_size, desired_size\n",
    "    resized_img = T.Resize((new_h, new_w))(img)\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "# Чтение исходного видео\n",
    "cap = cv2.VideoCapture(f\"{dataFolder}/src/Mating.mp4\")\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "out = cv2.VideoWriter(f\"{dataFolder}/result/3d_video.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 20.0, (frame_width, frame_height))\n",
    "\n",
    "# while True:\n",
    "for i in range(10):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Получение карты глубины для текущего кадра\n",
    "    depth_map = process_frame(frame, model)\n",
    "\n",
    "    # Здесь можно добавить код для создания 3D эффекта, используя карту глубины\n",
    "    # В этом примере мы просто сохраним оригинальный кадр\n",
    "    # Для демонстрации мы используем карту глубины как часть видео\n",
    "    # Чтобы видео не было статичным, добавим эффект параллакса\n",
    "    frame_with_depth = cv2.applyColorMap(depth_map, cv2.COLORMAP_JET)\n",
    "    out.write(frame_with_depth)\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\facebookresearch_WSL-Images_main\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Загрузка модели MiDaS для оценки глубины из PyTorch Hub\n",
    "model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "model.eval()\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "def resize_and_pad(img, base=32, desired_size=384):\n",
    "    w, h = img.size\n",
    "    new_w, new_h = desired_size, desired_size\n",
    "    resized_img = T.Resize((new_h, new_w))(img)\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "def process_frame(frame, model):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    frame_resized = resize_and_pad(frame_pil)\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    input_tensor = transform(frame_resized).unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "        depth_map = torch.nn.functional.interpolate(prediction.unsqueeze(1), size=frame.shape[:2], mode=\"bicubic\", align_corners=False).squeeze()\n",
    "        depth_map = depth_map.cpu().numpy()\n",
    "    depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    depth_map_normalized = depth_map_normalized.astype(np.uint8)\n",
    "    return depth_map_normalized\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"{dataFolder}/src/Mating.mp4\")\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "out = cv2.VideoWriter(f\"{dataFolder}/result/stereo_video.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 20.0, (frame_width * 2, frame_height))\n",
    "\n",
    "\n",
    "def create_stereo_frame(left_frame, shift):\n",
    "    # Для простоты, сдвигаем весь кадр влево для левого глаза и вправо для правого глаза\n",
    "    # Этот метод не учитывает реальную глубину объектов\n",
    "    right_frame = np.roll(left_frame, shift, axis=1)\n",
    "    left_frame = np.roll(left_frame, -shift, axis=1)\n",
    "    return np.concatenate((left_frame, right_frame), axis=1)\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    depth_map = process_frame(frame, model)\n",
    "    # Используйте карту глубины, чтобы определить, как сильно сдвигать пиксели\n",
    "    # Для примера, используем фиксированный сдвиг\n",
    "    shift = 30  # Фиксированное значение сдвига для демонстрации\n",
    "    stereo_frame = create_stereo_frame(frame, shift)\n",
    "\n",
    "    out.write(stereo_frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\facebookresearch_WSL-Images_main\n"
     ]
    }
   ],
   "source": [
    "# Упрощенный вариант с простым смещением пикселов\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Загрузка модели MiDaS для оценки глубины из PyTorch Hub\n",
    "model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "model.eval()\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "def resize_and_pad(img, base=32, desired_size=384):\n",
    "    w, h = img.size\n",
    "    new_w, new_h = desired_size, desired_size\n",
    "    resized_img = T.Resize((new_h, new_w))(img)\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "def process_frame(frame, model):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    frame_resized = resize_and_pad(frame_pil)\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    input_tensor = transform(frame_resized).unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "        depth_map = torch.nn.functional.interpolate(prediction.unsqueeze(1), size=frame.shape[:2], mode=\"bicubic\", align_corners=False).squeeze()\n",
    "        depth_map = depth_map.cpu().numpy()\n",
    "    depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    depth_map_normalized = depth_map_normalized.astype(np.uint8)\n",
    "    return depth_map_normalized\n",
    "\n",
    "\n",
    "def create_stereo_frame(left_frame, depth_map):\n",
    "    max_shift = 30  # Максимальный сдвиг для объектов на минимальной глубине\n",
    "    depth_map_scaled = (255 - depth_map) / 255  # Инвертирование и масштабирование карты глубины\n",
    "    shift_map = (depth_map_scaled * max_shift).astype(np.int32)\n",
    "\n",
    "    height, width = left_frame.shape[:2]\n",
    "    right_frame = np.zeros_like(left_frame)\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            shift = shift_map[y, x]\n",
    "            new_x = x + shift if x + shift < width else x - shift\n",
    "            right_frame[y, new_x] = left_frame[y, x]\n",
    "\n",
    "    left_frame_shifted = np.roll(left_frame, -max_shift // 2, axis=1)  # Простой сдвиг всего изображения\n",
    "    stereo_frame = np.concatenate((left_frame_shifted, right_frame), axis=1)\n",
    "    return stereo_frame\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"{dataFolder}/src/Mating.mp4\")\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "out = cv2.VideoWriter(f\"{dataFolder}/result/stereo_video.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 20.0, (frame_width * 2, frame_height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    depth_map = process_frame(frame, model)\n",
    "    stereo_frame = create_stereo_frame(frame, depth_map)\n",
    "    out.write(stereo_frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой модификации функции create_stereo_frame добавлена дополнительная функция fill_holes, которая пробегается по всем пикселям в смещенном правом изображении и, если находит \"дырку\" (пиксель, равный нулю), заполняет её значением ближайшего ненулевого пикселя. Этот подход довольно простой и может быть не идеален для всех сценариев, поскольку в реальности текстуры и детали объектов могут существенно отличаться, и более сложные методы интерполяции могут дать лучший визуальный результат.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\inimatic/.cache\\torch\\hub\\facebookresearch_WSL-Images_main\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Загрузка модели MiDaS для оценки глубины из PyTorch Hub\n",
    "model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "model.eval()\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "def resize_and_pad(img, base=32, desired_size=384):\n",
    "    w, h = img.size\n",
    "    new_w, new_h = desired_size, desired_size\n",
    "    resized_img = T.Resize((new_h, new_w))(img)\n",
    "    return resized_img\n",
    "\n",
    "\n",
    "def process_frame(frame, model):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    frame_resized = resize_and_pad(frame_pil)\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    input_tensor = transform(frame_resized).unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "        depth_map = torch.nn.functional.interpolate(prediction.unsqueeze(1), size=frame.shape[:2], mode=\"bicubic\", align_corners=False).squeeze()\n",
    "        depth_map = depth_map.cpu().numpy()\n",
    "    depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    depth_map_normalized = depth_map_normalized.astype(np.uint8)\n",
    "    return depth_map_normalized\n",
    "\n",
    "\n",
    "def fill_holes(interpolated_frame, depth_map):\n",
    "    h, w = depth_map.shape\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            if interpolated_frame[y, x].sum() == 0:  # Проверяем, является ли пиксель \"дыркой\"\n",
    "                # Ищем ближайший ненулевой пиксель\n",
    "                nx, ny = x, y\n",
    "                while nx < w - 1 and interpolated_frame[ny, nx].sum() == 0:\n",
    "                    nx += 1\n",
    "                if interpolated_frame[ny, nx].sum() != 0:\n",
    "                    interpolated_frame[y, x] = interpolated_frame[ny, nx]\n",
    "                else:\n",
    "                    nx, ny = x, y\n",
    "                    while nx > 0 and interpolated_frame[ny, nx].sum() == 0:\n",
    "                        nx -= 1\n",
    "                    if interpolated_frame[ny, nx].sum() != 0:\n",
    "                        interpolated_frame[y, x] = interpolated_frame[ny, nx]\n",
    "    return interpolated_frame\n",
    "\n",
    "\n",
    "def create_stereo_frame(left_frame, depth_map):\n",
    "    max_shift = 30  # Максимальный сдвиг\n",
    "    depth_map_scaled = (255 - depth_map) / 255  # Инвертируем и масштабируем карту глубины\n",
    "    shift_map = (depth_map_scaled * max_shift).astype(np.int32)\n",
    "\n",
    "    height, width = left_frame.shape[:2]\n",
    "    right_frame = np.zeros_like(left_frame)\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            shift = shift_map[y, x]\n",
    "            new_x = x + shift if x + shift < width else width - 1\n",
    "            right_frame[y, new_x] = left_frame[y, x]\n",
    "\n",
    "    # Заполнение дырок интерполяцией ближайшего соседа\n",
    "    right_frame_interpolated = fill_holes(right_frame, depth_map)\n",
    "\n",
    "    left_frame_shifted = np.roll(left_frame, -max_shift // 2, axis=1)\n",
    "    stereo_frame = np.concatenate((left_frame_shifted, right_frame_interpolated), axis=1)\n",
    "    return stereo_frame\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(f\"{dataFolder}/src/Mating.mp4\")\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "out = cv2.VideoWriter(f\"{dataFolder}/result/stereo_video2.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 20.0, (frame_width * 2, frame_height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    depth_map = process_frame(frame, model)\n",
    "    stereo_frame = create_stereo_frame(frame, depth_map)\n",
    "    out.write(stereo_frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
